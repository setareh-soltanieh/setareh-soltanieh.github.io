<!DOCTYPE HTML>
<!--
	Spectral by Pixelarity
	pixelarity.com @pixelarity
	License: pixelarity.com/license
-->
<html>
	<head>
		<link rel="icon" href="">
		<title>Setareh Soltanieh| Blog - SAIS </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                               tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                               });
        </script>
        <script type="text/javascript"
  			src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
		<script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-97381584-1', 'auto');
		  ga('send', 'pageview');

		</script>

		<style>
		* {
		  box-sizing: border-box;
		}

		.column {
		  float: left;
		  width: 50%;
		  padding: 5px;
		}

		/* Clearfix (clear floats) */
		.row::after {
		  content: "";
		  clear: both;
		  display: table;
		}
		</style>
		
		

	</head>
  
  
	<body>

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="../../index.html">Setareh Soltanieh</a></h1>
						<nav id="nav">
							<ul>
								<li class="special">
									<a href="#menu" class="menuToggle"><span>Menu</span></a>
									<div id="menu">
										<ul>
											<li><a href="../../index.html">Home</a></li>
											<li><a href="../../index.html#aboutme">About Me</a></li>
											<li><a href="../../index.html#experience">Experience</a></li>
											<li>&mdash;</li>
											<li><a href="../../publications.html">Projects</a></li>
											<li><a href="../index.html">Blog</a></li>
											<li><a href="../../datasets.html">Datasets</a></li>											
										</ul>
									</div>
								</li>
							</ul>
						</nav>
					</header>

	<!-- Main -->
	<article id="main">
		<section class="wrapper style5">

			<div class="inner">
			<h2>
			<center>
				Pre-positioning virtual objects at their intended locations in the real world environment
			</center>
			</h2>

<!--			<p>-->
<!--			<center>-->
<!--				Thursday, March 30th, 2023-->
<!--			</center>-->
<!--			</p>-->

			<p>
			<center>
				Setareh Soltanieh
			</center>
			</p>

    <h4>
    Enhancing Public Engagement in Urban Planning with AR
    </h4>    
        <p>
        A compelling example of pre-positioning virtual assets is the visualization of proposed urban buildings at their intended locations.
            Architects often struggle with securing permits and managing public feedback, which can lead to costly delays or cancellations.
            Traditional methods like signage and 2D renders fail to effectively engage the public, whereas AR enables municipalities to showcase the final design in real-world settings.
            This immersive experience helps stakeholders better understand the project’s impact.
            However, relying on pre-generated 3D maps for positioning is impractical in evolving environments like construction sites.
            Instead, using visual fiducial markers, such as AprilTags, offers a more adaptable solution, though challenges like high pedestrian and vehicle traffic must be considered to ensure accurate placement.
        </p>
        
    <h4>
    AR Pipeline for Pre-Positioning Virtual Objects
    </h4>
        <p>
        I have developed an AR pipeline that accurately pre-positions virtual objects at their intended locations in the real world.
            In this approach, users can view the virtual objects but cannot move them.
            The pipeline consists of four main modules: data capture, setting the origin, localization, and rendering.
            Each module plays a crucial role in ensuring precise alignment of virtual assets with the physical environment.
            This process is illustrated in the figure below.
        </p>

    <center>
	<img src="./images/AR_pipeline.png" style="width: 60%" />
	</center>
        
    <h4>
    Data Capturing
    </h4>
        <p>
        In this module, input data—including monocular camera frames and IMU measurements—are captured.
            The camera operates at 30Hz, while the IMU records data at 400Hz.
            To ensure reliable localization, both data sources must maintain stable frequencies, as missing camera frames can cause the algorithm to fail.
        </p>

    <h4>
    Setting the Origin
    </h4>
      <p>
      In this module, the origins of the real and virtual worlds are aligned.
          In AR applications, the device's initial pose is typically set as the origin, with all subsequent poses calculated relative to it.
          In this pipeline, an AprilTag serves as the common reference point for both worlds.
          At the start of the application, users scan the AprilTag, allowing the AR system to align its origin with the tag's position, ensuring accurate placement of virtual objects.
      </p>

    <h4>
    Localization
    </h4>
        <p>
        In this pipeline I used five state-of-the-art mono-inertial localization algorithms including: VINS-Mono, ORBSLAM3, Open-VINS, Kimera, and DM-VIO.
        </p>

    <h4>
    Rendering
    </h4>
        <p>
        In this pipeline a virtual cube has been rendered on the screen.
            This rendering is being conducted using OpenCV. At first the algorithm estimates whether or not the virutal
        </p>

    <h4>
    Results
    </h4>
      <h5>
      This pipeline has been tested for both indoor and outdoor environments.
      </h5>
        <p>
        For the indoor environments, I place the virtual cube at three different locations on a grid.
            As it is shown in the following Figures, the virtual cube is placed in (10, 2), (16, 14), and (60, 26).
        </p>
			
<!--		<center>-->
<!--		<img src="./images/AR_demo_1.png" width="100%" />-->
<!--		</center>-->
        <figure>
            <img src="./images/AR_demo_1.png" width="100%" alt="Augmented Reality Demo 1">
            <figcaption>The first demonstration of the AR experience.
                In this photo, the cube’s center is positioned at (10, 2).</figcaption>
        </figure>

        <figure>
            <img src="./images/AR_demo_1.png" width="100%" alt="Augmented Reality Demo 2">
            <figcaption>The first demonstration of the AR experience.
                In this photo, the cube’s center is positioned at (16, 14).</figcaption>
        </figure>

        <figure>
            <img src="./images/AR_demo_1.png" width="100%" alt="Augmented Reality Demo 3">
            <figcaption>The first demonstration of the AR experience.
                In this photo, the cube’s center is positioned at (60, 26).</figcaption>
        </figure>
				
		<p>
        For the outdoor environments, I have placed the virtual cube at (0, 0) and (0, 5) locations.
        </p>
        <video width="100%" controls>
            <source src="./videos/Outdoor_0_0.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>

        <video width="100%" controls>
            <source src="./videos/Outdoor_0_5.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>

      <h5>
      Using this AR pipeline, we can reliably pre-position virtual objects in the environment.
      </h5>	
<!--        <p>-->
<!--        We then trained SAIS to distinguish between low and high skill activity, for two distinct surgical activities: needle handling and needle driving.-->
<!--        For needle handling, a low-skill assessment is based on the number of times a surgeon had to reposition their grasp of the needle (more repositions = lower quality). -->
<!--        For needle driving, a low-skill assessment is based on the smoothness with which it was performed (less smooth = lower quality). -->
<!--        </p>-->
<!--        <p>-->
<!--        We show that SAIS generalizes to unseen videos, surgeons, and hospitals. -->
<!--        This is evident by its strong performance (AUC > 0.80) across the skill assessment tasks and hospitals.  -->
<!--        SAIS also naturally lends itself to explainable findings. -->
<!--        Below, we illustrate the relative importance of individual frames in a surgical video, as identified by SAIS, and-->
<!--        show that the frames with the highest level of importance do indeed align with the ground-truth low-skill assessment of the surgical activity.-->
<!--        </p>-->
<!--			-->
<!--		<center>-->
<!--		<img src="./images/skills_assessment.jpg" width="80%" /> 									-->
<!--		</center>-->

<!--		<h4>-->
<!--    Combining the elements of surgery-->
<!--    </h4>		-->
<!--      <h5>-->
<!--      How do we practically leverage the findings of this study?  -->
<!--      </h5>	-->
<!--        <p>-->
<!--        Thus far, we have demonstrated SAIS' ability to independently decode multiple elements of surgical activity (steps, gestures, and skill-levels). -->
<!--        Considering these elements in unison, however, suggests that SAIS can provide a surgeon with performance feedback of the following form: -->
<!--        </p>-->

<!--        <p style="background-color: #eff5fb; padding: 10px; border: 1px solid blue;">-->
<!--        When completing stitch number 3 of the suturing step, your needle handling (what - <i>sub-phase</i>) was executed poorly (how - <i>skill</i>). -->
<!--        This is likely due to your activity in the first and final quarters of the needle handling sub-phase (why - <i>explanation</i>). -->
<!--        </p>-->

<!--        <p>-->
<!--        Such granular and temporally-localized feedback allows a surgeon to better focus on the element of surgery that requires improvement. -->
<!--        As such, a surgeon can now better identify, and learn to avoid, problematic intra-operative surgical behaviour in the future.-->
<!--        </p>					-->

<!--		<h4>-->
<!--    Outlining the translational impact-->
<!--    </h4>		-->
<!--      <h5>-->
<!--      What are the implications of our study?-->
<!--      </h5>	-->
<!--        <p>-->
<!--        The ability of SAIS to generalize to videos across a wide range of settings (e.g., surgeons, hospitals, and surgical procedures) is likely to instill surgeons with greater confidence in its performance. -->
<!--        This opens the door for the large-scale decoding of the millions of surgical videos recorded on an annual basis. -->
<!--        Moreover, SAIS' ability to provide explainable findings, by highlighting relevant video frames, has a threefold benefit. -->
<!--        First, explainability can improve the trustworthiness of an AI system, a critical pre-requisite for clinical adoption. -->
<!--        Second, it allows for the provision of more targeted feedback to surgeons about their performance, thereby allowing them to modulate their behaviour to improve patient outcomes.-->
<!--        Third, explainability can improve the transparency of an AI system, and thus contributes to the ethical deployment of AI systems in a clinical setting.-->
<!--        </p>-->
<!--				-->
<!--		<h4>-->
<!--    Moving forward-->
<!--    </h4>		-->
<!--      <h5>-->
<!--      Where do we go from here?  -->
<!--      </h5>	-->
<!--        <p>-->
<!--        Beyond demonstrating the reliability of SAIS on data from distinct settings, it is critical that we also explore the ethical implications of its deployment.-->
<!--	To that end, in a concurrent study published in <a href="https://www.nature.com/articles/s41746-023-00766-2" style="color:#0e8ec9;"><u>npj Digital Medicine</u></a>, we explore the fairness of SAIS' skill assessments and whether it disadvantages one surgeon subcohort over another.-->
<!--        One way to leverage SAIS is for the provision of surgeon feedback, and we see its explanations playing a critical role in that process. -->
<!--	As such, in a concurrent study published in <a href="https://www.nature.com/articles/s43856-023-00263-3" style="color:#0e8ec9;"><u>Communications Medicine</u></a>, we systematically investigate the quality and fairness of SAIS' explanations. -->
<!--        </p>-->
<!--				-->
<!--		<h4>-->
<!--		Acknowledgements-->
<!--		</h4>-->
<!--		<p>  -->
<!--		We would like to thank <a id='overwhite' href="https://www.youtube.com/watch?v=ZFkV6mwHhX0" style="color:#0e8ec9;"><u>Wadih El Safi</u></a> for lending us his voice. -->
<!--		</p>-->
				
		</br>
							
					</div>
				</section>
			</article>
    
		
			</div>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/jquery.scrollex.min.js"></script>
			<script src="../../assets/js/jquery.scrolly.min.js"></script>
			<script src="../../assets/js/skel.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../../assets/js/main.js"></script>
				
				
  	</body>
	
</html>
